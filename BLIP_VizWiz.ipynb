{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTING LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aUDd8A8I5vfl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from PIL import Image\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8SYVYl695vfn"
   },
   "outputs": [],
   "source": [
    "class CustomBLIPDataset(Dataset):\n",
    "    def __init__(self, json_file, image_dir, processor, max_length=32):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.samples = []\n",
    "        for ann in data['annotations']:\n",
    "            self.samples.append({\n",
    "                'image_id': ann['image_id'],\n",
    "                'caption': ann['caption']\n",
    "            })\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image_id = sample['image_id']\n",
    "        caption = sample['caption']\n",
    "        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NjfYSXIR5vfo"
   },
   "outputs": [],
   "source": [
    "class BLIPTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"Salesforce/blip-image-captioning-base\",\n",
    "        train_batch_size=8,\n",
    "        eval_batch_size=16,\n",
    "        learning_rate=5e-5,\n",
    "        num_epochs=10,\n",
    "        warmup_steps=1000,\n",
    "        max_length=32,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        output_dir='blip_checkpoints'\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def prepare_data(self, json_path, image_dir, train_ratio=0.8, val_ratio=0.1):\n",
    "        full_dataset = CustomBLIPDataset(\n",
    "            json_file=json_path,\n",
    "            image_dir=image_dir,\n",
    "            processor=self.processor,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(train_ratio * total_size)\n",
    "        val_size = int(val_ratio * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "\n",
    "        print(f\"Dataset sizes - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "\n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            full_dataset,\n",
    "            [train_size, val_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        self.val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.eval_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        self.test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.eval_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        return self.train_dataloader, self.val_dataloader, self.test_dataloader\n",
    "\n",
    "    def compute_metrics(self, pred_captions, true_captions):\n",
    "        # Calculate BLEU score\n",
    "        references = [[caption.split()] for caption in true_captions]\n",
    "        predictions = [caption.split() for caption in pred_captions]\n",
    "        bleu_score = corpus_bleu(references, predictions)\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = {\n",
    "            'rouge1': 0.0,\n",
    "            'rouge2': 0.0,\n",
    "            'rougeL': 0.0\n",
    "        }\n",
    "        for pred, ref in zip(pred_captions, true_captions):\n",
    "            scores = self.rouge_scorer.score(ref, pred)\n",
    "            rouge_scores['rouge1'] += scores['rouge1'].fmeasure\n",
    "            rouge_scores['rouge2'] += scores['rouge2'].fmeasure\n",
    "            rouge_scores['rougeL'] += scores['rougeL'].fmeasure\n",
    "        num_samples = len(pred_captions)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key] /= num_samples\n",
    "        return {\n",
    "            'bleu': bleu_score,\n",
    "            **rouge_scores\n",
    "        }\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        total_steps = len(self.train_dataloader) * self.num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\nStarting epoch {epoch + 1}/{self.num_epochs}\")\n",
    "\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            progress_bar = tqdm(self.train_dataloader, desc=f\"Training epoch {epoch + 1}\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                pixel_values = batch['pixel_values'].to(self.device)\n",
    "                attention_mask = batch.get('attention_mask', None)\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = attention_mask.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            avg_train_loss = train_loss / len(self.train_dataloader)\n",
    "\n",
    "            # Validation\n",
    "            val_metrics = self.evaluate(self.val_dataloader, \"Validation\")\n",
    "\n",
    "            metrics = {\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_bleu': val_metrics['bleu'],\n",
    "                'val_rouge1': val_metrics['rouge1'],\n",
    "                'val_rouge2': val_metrics['rouge2'],\n",
    "                'val_rougeL': val_metrics['rougeL']\n",
    "            }\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} metrics:\", metrics)\n",
    "\n",
    "            if val_metrics['loss'] < best_val_loss:\n",
    "                best_val_loss = val_metrics['loss']\n",
    "                self.save_model(f\"{self.output_dir}/best_model\")\n",
    "                print(f\"New best model saved with validation loss: {val_metrics['loss']:.4f}\")\n",
    "            self.save_model(f\"{self.output_dir}/checkpoint_epoch_{epoch + 1}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader, split=\"Validation\"):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_references = []\n",
    "\n",
    "        progress_bar = tqdm(dataloader, desc=f\"{split} evaluation\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            pixel_values = batch['pixel_values'].to(self.device)\n",
    "            attention_mask = batch.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "\n",
    "            total_loss += outputs.loss.item()\n",
    "\n",
    "            generated_ids = self.model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                max_length=self.max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            generated_captions = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            reference_captions = self.processor.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "            all_predictions.extend(generated_captions)\n",
    "            all_references.extend(reference_captions)\n",
    "\n",
    "        metrics = self.compute_metrics(all_predictions, all_references)\n",
    "        metrics['loss'] = total_loss / len(dataloader)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def test(self):\n",
    "        test_metrics = self.evaluate(self.test_dataloader, \"Test\")\n",
    "        print(f\"Test metrics:\", test_metrics)\n",
    "        return test_metrics\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'processor_state': self.processor.save_pretrained,\n",
    "            'model_name': self.model_name\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        checkpoint = torch.load(filename, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model_name = checkpoint['model_name']\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469,
     "referenced_widgets": [
      "40e355489ea644db9dea16c2cdb36b44",
      "633a7ed870ba48eea03ba6962a0dc3f7",
      "84f25c6d3ec14c469126cbd683cc2c7e",
      "fda13a2916794381b9671767422fcf99",
      "0060b282e2c846aa888ff4233f6e86bd",
      "8c933b103b1740b590f1235d21a0691c",
      "07f8c0b4c611407db4bc5535a3009078",
      "0b99c60eca44442d90b0dbacefe467f8",
      "fdbd1ba4f1c442dfa1ca90aff985bbcf",
      "260540eb10c84b12836bcf4dcf2ef29c",
      "f0acfc4ebdbf43af96163ce75fd202f8",
      "e3b88c59cc7b4b6bb9aaf507185e15ef",
      "99d1fc13f399408ea5359ea94da1c0c7",
      "5c7c730f847d462681a541b08c0a21eb",
      "0503c402ebfb4002980351d142eb82ff",
      "cedc55bde1f443e6872607cebab5cdbc",
      "80a18bfb81684343a85c5125b68edba7",
      "bd0ce23272414f5598d5375f6aff7aa0",
      "4c819e13e6cd4d738ae405bbcddf012b",
      "f27f674fd776472ab22f80c7b4719188",
      "6dac31c636dc4cfb9f5c21ca47063bbb",
      "300a391c126d4b919373284b27bc1fc2",
      "2227d819c4ee4f44a2c20a13a2a54045",
      "56da6aaefee046aa835a5ba11d547ac9",
      "3d0f1175e8d64779ac44cf1115a587b1",
      "f360dbefbfab45d6b878d07d427b810c",
      "fb528628b8b745629bc45bcd55c4592a",
      "a16f7777026a496f94cb870d8f49c605",
      "571fa727b99b4a79816342aba4a45874",
      "3e23bda3f3e54149890d393fe66b2b85",
      "bdcdb4f14b2d42db8d857b2df4f70811",
      "49aae9ca1d0943498fadd551034c205b",
      "d546eed3b05a4734a17b54943623e8c2",
      "2c212f5cda074e9d994597f6dbc16fed",
      "9aafd612c69947b9ac58ae21cd393f10",
      "adc3572369fc47e58bffb757de938b65",
      "5ca4ada279fa4955965f440645bf6cf1",
      "0bc1eba7e4354ff3be8301634ff46193",
      "97b523215426406097d5addb5167164b",
      "cb2c7edbe2824ad5b5555371970110c7",
      "00d722605ba34badbd4b3edc73c71d3a",
      "f114843292b44d189757221d4c522c3c",
      "27670c6ec6454f0699ef054b222e4947",
      "8b9ab0e532644d51ab0a5c08e09408a9",
      "a4bdbdd4998d492aa692432cd6b378f5",
      "f32aeb07c9594336bb2d8bd7d2c05774",
      "e78938181a8f409f94365e9a0918e6f3",
      "d581909c0c8e4ee194d3e5063429d98e",
      "c53591832d364141a819f30c92d1b002",
      "881e32ccac2648338364178a006a8f42",
      "c0aed05e3a464dc1a9dd5eb41a026e20",
      "7e8929feb5104d98b594ef8e6ff4901d",
      "744f8da7d27643989104c481a5d5e0a8",
      "193b68c4c6aa4075915dcba33cdf9039",
      "b7f94b81112048538ed699560167cc12",
      "e84ff46ec27b487f87d41bab7325c34e",
      "55266af2724e46428f3cff8bee3622a5",
      "9bf08346f5bb4fa483a90188ac2e6f88",
      "407cec9354ac4d9bbec7e5734063cd2d",
      "ec2a89ee6df0431f986d2326c642f20b",
      "9d4492ba4be3485ea5c51ef6b4b5fe8d",
      "94d5c38629be407aadac37e83f1a6877",
      "8cdb3b8bb8ce4a78ac174f3fd48f6db2",
      "916b47e97e1b4dff8b98247df94de294",
      "078a94ab662f4d8ba6c094c5d1b9946d",
      "fb1d4308c9a14bc6b3024a0fd5229933",
      "8df82591772649feac9f4a5c5d00ad53",
      "329638306d4540039be7e9bd4ebc5d64",
      "62b35855fff849a1ac5ce0d9c8d6561d",
      "63058e02d88b45cda5c8d12ba7d66a82",
      "32b463f06e8846809d1932fb5e6c252b",
      "f101e9f8e28a4058a6b9962a61cf20f1",
      "f0322c4eafd24ed8a04db1c34c4baa96",
      "a7c06e6878264362acb4d98831a6899d",
      "5108a0be13bc4e0d997e28b108952762",
      "68ebf0be3aa544649303a74f7ee8e602",
      "68d548af31734787b1c32ebaf5967843"
     ]
    },
    "id": "56mtBsgd5vfq",
    "outputId": "7a9436cf-3220-4e6a-e0a3-78a19f33611d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset sizes - Train: 2000, Val: 250, Test: 250\n"
     ]
    }
   ],
   "source": [
    "trainer = BLIPTrainer(\n",
    "    model_name=\"Salesforce/blip-image-captioning-base\",\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_epochs=10,\n",
    "    warmup_steps=1000,\n",
    "    max_length=32,\n",
    "    output_dir='blip_checkpoints'\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = trainer.prepare_data(\n",
    "    json_path='images.json',\n",
    "    image_dir='images',\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2njT_Y_5vfs",
    "outputId": "c2417833-6882-4cf0-e7cc-9234c0d5cfd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 250/250 [05:00<00:00,  1.20s/it, loss=3.79]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:11<00:00,  8.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 metrics: {'train_loss': 6.141959931373596, 'val_loss': 3.811283975839615, 'val_bleu': 0.08775112768060989, 'val_rouge1': 0.15882476738688986, 'val_rouge2': 0.12781450853520607, 'val_rougeL': 0.15387159506901166}\n",
      "Model saved to blip_checkpoints/best_model\n",
      "New best model saved with validation loss: 3.8113\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_1\n",
      "\n",
      "Starting epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 250/250 [05:03<00:00,  1.21s/it, loss=1.01]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:17<00:00,  8.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 metrics: {'train_loss': 1.742139730811119, 'val_loss': 1.2196671664714813, 'val_bleu': 0.12549425110812118, 'val_rouge1': 0.2955645744029393, 'val_rouge2': 0.16630010683776775, 'val_rougeL': 0.2686275117736633}\n",
      "Model saved to blip_checkpoints/best_model\n",
      "New best model saved with validation loss: 1.2197\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_2\n",
      "\n",
      "Starting epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 250/250 [05:03<00:00,  1.21s/it, loss=0.936]\n",
      "Validation evaluation: 100%|██████████| 16/16 [01:54<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 metrics: {'train_loss': 0.9558675435781478, 'val_loss': 1.1771351844072342, 'val_bleu': 0.11596268235628784, 'val_rouge1': 0.27993081614563514, 'val_rouge2': 0.1546276744252678, 'val_rougeL': 0.25455645865289744}\n",
      "Model saved to blip_checkpoints/best_model\n",
      "New best model saved with validation loss: 1.1771\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_3\n",
      "\n",
      "Starting epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 250/250 [05:03<00:00,  1.21s/it, loss=0.806]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:21<00:00,  8.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 metrics: {'train_loss': 0.7763670245409012, 'val_loss': 1.1783964596688747, 'val_bleu': 0.11987048823038354, 'val_rouge1': 0.26725660665003764, 'val_rouge2': 0.15605687311310165, 'val_rougeL': 0.24812848601448573}\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_4\n",
      "\n",
      "Starting epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 250/250 [05:02<00:00,  1.21s/it, loss=0.54]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:16<00:00,  8.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 metrics: {'train_loss': 0.6025062891244888, 'val_loss': 1.228714156895876, 'val_bleu': 0.12027737961385931, 'val_rouge1': 0.31319443262298247, 'val_rouge2': 0.1637502339826814, 'val_rougeL': 0.27944674810300973}\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_5\n",
      "\n",
      "Starting epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 250/250 [05:03<00:00,  1.21s/it, loss=0.31]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:15<00:00,  8.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 metrics: {'train_loss': 0.4066934297084808, 'val_loss': 1.2845053859055042, 'val_bleu': 0.1159657336296715, 'val_rouge1': 0.33710858786786063, 'val_rouge2': 0.15615731703470428, 'val_rougeL': 0.2979902985199454}\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_6\n",
      "\n",
      "Starting epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 250/250 [05:02<00:00,  1.21s/it, loss=0.388]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:17<00:00,  8.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 metrics: {'train_loss': 0.262628405213356, 'val_loss': 1.3757603019475937, 'val_bleu': 0.09932028079526377, 'val_rouge1': 0.3261094840564905, 'val_rouge2': 0.13346278278586304, 'val_rougeL': 0.2819953558372457}\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_7\n",
      "\n",
      "Starting epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8: 100%|██████████| 250/250 [05:02<00:00,  1.21s/it, loss=0.187]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:24<00:00,  9.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 metrics: {'train_loss': 0.17672417384386063, 'val_loss': 1.435774713754654, 'val_bleu': 0.08205557206418007, 'val_rouge1': 0.3054646386785037, 'val_rouge2': 0.11439032932115378, 'val_rougeL': 0.2647234504782206}\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_8\n",
      "\n",
      "Starting epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9: 100%|██████████| 250/250 [05:02<00:00,  1.21s/it, loss=0.133]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:19<00:00,  8.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 metrics: {'train_loss': 0.12455823394656182, 'val_loss': 1.5176988244056702, 'val_bleu': 0.07525727109097474, 'val_rouge1': 0.28785992290393553, 'val_rouge2': 0.10286787225701541, 'val_rougeL': 0.24671647870046157}\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_9\n",
      "\n",
      "Starting epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 10: 100%|██████████| 250/250 [05:02<00:00,  1.21s/it, loss=0.0944]\n",
      "Validation evaluation: 100%|██████████| 16/16 [02:31<00:00,  9.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 metrics: {'train_loss': 0.09534445084631443, 'val_loss': 1.5522729605436325, 'val_bleu': 0.08777703652939359, 'val_rouge1': 0.29933885036681745, 'val_rouge2': 0.1153205375408667, 'val_rougeL': 0.2555412900249298}\n",
      "Model saved to blip_checkpoints/checkpoint_epoch_10\n",
      "Model saved to fine-tuned_BLIP.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model('fine-tuned_BLIP.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uzvfn1RgMB_",
    "outputId": "cdd4de13-7204-4716-a630-c8a7bcaeadbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test evaluation: 100%|██████████| 16/16 [02:34<00:00,  9.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'bleu': 0.09442623168048496, 'rouge1': 0.2932291978870383, 'rouge2': 0.1275179068814095, 'rougeL': 0.2548687881025692, 'loss': 1.4956378638744354}\n",
      "\n",
      "Evaluation on test set:\n",
      " {'bleu': 0.09442623168048496, 'rouge1': 0.2932291978870383, 'rouge2': 0.1275179068814095, 'rougeL': 0.2548687881025692, 'loss': 1.4956378638744354}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.test()\n",
    "print(\"\\nEvaluation on test set:\\n\", test_metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
